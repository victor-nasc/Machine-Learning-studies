{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory\n",
    "### Made by: Victor Nascimento Ribeiro 07/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">\n",
    "Training dataset: $D = {(x_i, y_i) : i = 1, . . . , N}$\n",
    "    \n",
    "Target: $y = \\{\\color{red}{-1},\\color{blue}{+1}\\}$\n",
    "\n",
    "Hypothesis space: $H$\n",
    "\n",
    "Choose $g \\in H$ that minimizes some error measure $E_{in}$ over $D$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is Learning Feasible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hoeffding inequality for finite hypothesis set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the chosen hypothesis $g$ behave <ins>out of sample</ins>?\n",
    "\n",
    "- in Sample Error: $\\normalsize E_{in}$ (empirical error)\n",
    "- out of Sample Error: $\\normalsize E_{out}$ (true error)\n",
    "\n",
    "<font size=\"3\">\n",
    "$E_{in}$ is an estimate of $E_{out}$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Central Limit Theorem\n",
    "\n",
    "- Take samples of size $N$ and compute the fraction $E_{in}$\n",
    "- Repeat this several times $\\; \\rightarrow \\; \\infty$\n",
    "- The distribution of $E_{in}$ will be a normal distribution with mean $E_{out}$\n",
    "- The larger $N$, the smaller the standard deviation of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hoeffding inequality\n",
    "\n",
    "$\\large P(|E_{in} - E_{out}| > \\epsilon) \\leq 2 e^{\\Large -2 \\epsilon^{2} N}$\n",
    "\n",
    "where $\\large \\epsilon$ is the maximum error tolered\n",
    "\n",
    "<img src=\"images/Hoeffding.png/\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem\n",
    "\n",
    "We can not simply apply Hoeffding to the chosen hypothesis $g$ <br />\n",
    "**WHY?** Because $g$ is not a fixed hypothesis; it is a chosen one <br />\n",
    "we need to **choose** from multiple $ h's$\n",
    "\n",
    "<br />\n",
    "\n",
    "We should consider the probability of some htpothesis $h_m$ be such that : $\\normalsize |E_{in}(h_m) - E_{out}(h_m)| > \\epsilon$\n",
    "\n",
    "If we have $M$ hypothesis $h_1, h_2, ..., h_M$, and we choose one, which we denote $g$\n",
    "\n",
    "$ \\begin{split} \\normalsize\n",
    "P(|E_{in}(g) - E_{out}(g)| > \\epsilon) & \\leq\n",
    "\\sum_{m = 1}^{M} P(|E_{in}(h_m) - E_{out}(h_m)| > \\epsilon) \\\\ & \\leq \n",
    "\\sum_{m = 1}^{M} 2 e^{\\normalsize -2 \\epsilon^{2} N}\n",
    "\\end{split}$\n",
    "\n",
    "Thus: $\\large P(|E_{in}(g) - E_{out}(g)| > \\epsilon) \\leq 2 \\color{red}{M} e^{\\normalsize -2 \\epsilon^{2} N}$\n",
    "\n",
    "<br />\n",
    "\n",
    "**Consistent** with our intuition: <br />\n",
    "• negative exponential $\\rightarrow$ larger $N$ implies <ins>smaller</ins> bound\n",
    "\n",
    "**Contrary** to our intuition: <br />\n",
    "• number of hypothesis $M$ $\\rightarrow$ the larger the hypothesis space $H$, the <ins>larger</ins> the bound\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Should we, then, choose a small hypothesis space ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The choice of $g$ from $H$ is affected by $D$ (training data)\n",
    "\n",
    "- Usually there are many similar hypothesis $h_j$ that classify samples in $D$ in the exact same way\n",
    "\n",
    "- If an hypothesis corresponds to a “bad” event, would it not be reasonable to think that other similar hypothesis also correspond to a “bad” event\n",
    "\n",
    "To improve the bound, we will replace the <ins>Union bound</ins> with one that takes the overlap into consideration\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important definitions:**\n",
    "\n",
    "$\\normalsize H(x_1,x_2,...,x_N) = \\big\\{(h(x_1),h(x_2),...,h(x_N)\\big\\} \\\\ \n",
    "\\normalsize h(x) \\leftarrow \\{-1,+1\\}$, set of all hypothesys, $\\; |H|$ can be infinity\n",
    "\n",
    "<br />\n",
    "\n",
    "- **Dichotomy** <br /> \n",
    "Ways that the data can be separated (combinations)<br />\n",
    "Number of dichotomies $|H(x_1,x_2,...,x_N)|$ is at most $2^N$ <br />\n",
    "<br />\n",
    "- **Growth function** <br />\n",
    "The growth function “groups hypotheses” according to their behavior on $D$ <br />\n",
    "The growth function counts the maximum number of dichotomies on any N points <br /> \n",
    "<ins>Candidate for replacing</ins> $\\color{red}M$ <br />\n",
    "$ m_H(N) =  \\underset{x_1,x_2,...,x_N \\in X}{max} |H(x_1,x_2,...,x_N)|  \\leq 2^N $ <br />\n",
    "<br />\n",
    "- **Break point** <br />\n",
    "Point where $ m_H(N) < 2^N$ <br />\n",
    "There may be no breakpoint <br />\n",
    "After it the _Growth function_ is polinomial<br />\n",
    "\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Why Growth function is a good candidate ?? <ins> YES</ins>!! <br />\n",
    "$m_H(N)$ is <ins>polinomial</ins> if there is a break point<br />\n",
    "When there is a break point $k$, the effective number of hypothesis is bounded by a polynomial of order $N^{k-1}$<br />\n",
    "$m_H(N)$ tends to zero by increasing N\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VC inequality** (classification) <br />\n",
    "Assuming $|E_{in}(h) - E_{out}(h)| \\approx |E_{in}(h) - E_{in}'(h)|$ <br />\n",
    "$\\normalsize P(|E_{in}(g) - E_{out}(g)| > \\epsilon) \\leq \\color{red}{4} m_H(\\color{red}{2N}) e^{\\normalsize -\\color{red}{\\frac{1}{8}} \\epsilon^{2} N}$\n",
    "\n",
    "<br />\n",
    "\n",
    "- $\\color{red}{2N}$: <br />\n",
    "hypotheses are grouped based on their behavior on D, but their behavior outside $D$ is not the same <br />\n",
    "to track $|E_{in}(h) - E_{out}(h)|  > \\epsilon$, we track $|E_{in}(h) - E_{in}'(h)|$ (relative to $D$ and $D'$, both of size N)\n",
    "\n",
    "- $\\color{red}{4}$ and $\\color{red}{\\frac{1}{8}}$: <br />\n",
    "these are factors to account for the uncertainties added when we replace $|E_{in}(h) - E_{out}(h)|  > \\epsilon$ with $|E_{in}(h) - E_{in}'(h)|  > \\epsilon$\n",
    "\n",
    "<br />\n",
    "\n",
    "<ins>VC Dimention</ins>: The largest number of points that can be shattered by $H$ (\"Degrees of freedom\")<br />\n",
    "If $k$ is a break point for $H$, then $d_{VC}(H) < k$, <br />\n",
    "$d_{VC}(H) + 1$ is a break point\n",
    "\n",
    "- Ex.: for perceptrons $d_{VC} = d+1 \\;$, number of parameters $w_0,w_1,...,w_d$\n",
    "\n",
    "<br />\n",
    "\n",
    "In terms of the $VC$ dimension $d_{VC}$: <br />\n",
    "$ m_H(N) \\leq N^{d_{VC}}+1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VC Bound**\n",
    "\n",
    "$\\normalsize P(|E_{in}(g) - E_{out}(g)| > \\epsilon) \\leq 4 m_H(2N) e^{\\normalsize -\\frac{1}{8}\\epsilon^{2} N}$<br />\n",
    "$\\normalsize \\delta = 4m_H(2N) e^{\\normalsize -\\frac{1}{8}\\epsilon^{2} N} \\implies \\epsilon = \\sqrt{\\frac{8}{N} \\ln{\\frac{4m_H(2N)}{\\delta}}}$\n",
    "\n",
    "where $\\Omega(N,H,\\delta)$ is the $\\text{Generalization error}$\n",
    "\n",
    "thus:\n",
    "\n",
    "$\\normalsize E_{out} \\leq E_{in} + \\underbrace{\\sqrt{\\frac{8}{N} \\ln{\\frac{4m_H(2N)}{\\delta}}}}_{\\Large\\Omega(N,H,\\delta)}$\n",
    "\n",
    "<br />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bias Variance analysis** (regression) <br />\n",
    "$\\normalsize E_{out} = bias + variance$ <br />\n",
    "- $\\mathbf{bias}$ refers to an average hypothesis $\\bar{g}$\n",
    "- analisys using all data-sets in $H$\n",
    "\n",
    "where $bias \\approx E_{in} \\; and \\; variance \\approx \\Omega(N,H,\\delta)$ (similar)\n",
    "\n",
    "\n",
    "<img src=\"images/Bias1.png/\" width= \"500\">\n",
    "<img src=\"images/Bias2.png/\" width= \"500\">\n",
    "<img src=\"images/Bias3.png/\" width= \"500\">\n",
    "<img src=\"images/Bias4.png/\" width= \"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRADE OFF\n",
    "<ins>LARGER</ins> $\\text{Hypothesis space} \\implies $<ins>LARGER</ins> $E_{in}$ or $bias$ $\\implies$ <ins>LARGER</ins> $\\Omega$ or $variance$\n",
    "\n",
    "thus the expressiveness of $H$ should be matched to the amount of available data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    " - Abu-Mostafa, Yaser S., Magdon-Ismail, Malik and Lin, Hsuan-Tien. Learning From Data. : AMLBook, 2012.\n",
    " - https://work.caltech.edu/telecourse (lectures 5-8)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
